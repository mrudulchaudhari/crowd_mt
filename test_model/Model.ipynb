{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br6jzJFiEclR",
        "outputId": "fa3df85c-6a17-4f42-84be-58a7b8176069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset generation with binary columns...\n",
            "--- Generating dataset for: Lalbaugcha Raja, Mumbai ---\n",
            "Successfully created 'lalbaugcha_raja_binary_dataset.csv' with 3960 rows.\n",
            "\n",
            "Preview of the new format:\n",
            "       mandal_name    city            datetime  year  day_of_festival  \\\n",
            "0  Lalbaugcha Raja  Mumbai 2010-09-05 00:00:00  2010                1   \n",
            "1  Lalbaugcha Raja  Mumbai 2010-09-05 01:00:00  2010                1   \n",
            "2  Lalbaugcha Raja  Mumbai 2010-09-05 02:00:00  2010                1   \n",
            "3  Lalbaugcha Raja  Mumbai 2010-09-05 03:00:00  2010                1   \n",
            "4  Lalbaugcha Raja  Mumbai 2010-09-05 04:00:00  2010                1   \n",
            "\n",
            "   hour_of_day  is_weekend  is_special_day     weather  headcount  \n",
            "0            0           1               1       Humid      79170  \n",
            "1            1           1               1  Heavy Rain       4376  \n",
            "2            2           1               1       Sunny      13827  \n",
            "3            3           1               1      Cloudy      12522  \n",
            "4            4           1               1  Heavy Rain      12776  \n",
            "\n",
            "\n",
            "--- Generating dataset for: Dagdusheth Halwai Ganpati, Pune ---\n",
            "Successfully created 'dagdusheth_halwai_ganpati_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Andhericha Raja, Mumbai ---\n",
            "Successfully created 'andhericha_raja_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Kasba Ganpati, Pune ---\n",
            "Successfully created 'kasba_ganpati_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Siddhivinayak Temple, Mumbai ---\n",
            "Successfully created 'siddhivinayak_temple_binary_dataset.csv' with 3960 rows.\n",
            "All datasets have been generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- Main Generation Function ---\n",
        "def generate_mandal_dataset(mandal_name, city, base_headcount_range, yearly_growth_factor, start_year=2010, num_years=15):\n",
        "    \"\"\"\n",
        "    Generates a realistic dummy dataset with binary flags for weekend and special days.\n",
        "    \"\"\"\n",
        "    print(f\"--- Generating dataset for: {mandal_name}, {city} ---\")\n",
        "\n",
        "    FESTIVAL_DURATION_DAYS = 11\n",
        "    weather_options = ['Humid', 'Cloudy', 'Sunny', 'Light Rain', 'Heavy Rain']\n",
        "    all_data = []\n",
        "    total_hours = FESTIVAL_DURATION_DAYS * 24\n",
        "\n",
        "    for year_offset in range(num_years):\n",
        "        current_year = start_year + year_offset\n",
        "        festival_start_date = datetime(current_year, 9, 5, 0, 0, 0)\n",
        "\n",
        "        for hour_offset in range(total_hours):\n",
        "            current_time = festival_start_date + timedelta(hours=hour_offset)\n",
        "\n",
        "            day_of_festival = (current_time - festival_start_date).days + 1\n",
        "            hour_of_day = current_time.hour\n",
        "\n",
        "            # --- MODIFICATION START ---\n",
        "\n",
        "            # 1. Determine day of week for headcount logic, then create binary flag\n",
        "            day_of_week_str = current_time.strftime('%A')\n",
        "            is_weekend = 1 if day_of_week_str in ['Saturday', 'Sunday'] else 0\n",
        "\n",
        "            # 2. Determine special day type for headcount logic, then create binary flag\n",
        "            if day_of_festival == 1: special_day_str = 'Pratishthapana'\n",
        "            elif day_of_festival == 11: special_day_str = 'Visarjan'\n",
        "            elif day_of_festival in [5, 7, 10]: special_day_str = 'Key Day'\n",
        "            else: special_day_str = 'Regular Day'\n",
        "            is_special_day = 1 if special_day_str != 'Regular Day' else 0\n",
        "\n",
        "            # --- MODIFICATION END ---\n",
        "\n",
        "            weather = random.choice(weather_options)\n",
        "\n",
        "            # Headcount Simulation (uses the original string variables for logic)\n",
        "            base_headcount = random.randint(*base_headcount_range)\n",
        "            base_headcount += (current_year - start_year) * yearly_growth_factor\n",
        "            headcount = float(base_headcount)\n",
        "\n",
        "            if 18 <= hour_of_day <= 23: headcount *= random.uniform(2.5, 4.0)\n",
        "            elif 1 <= hour_of_day <= 5: headcount *= random.uniform(0.1, 0.4)\n",
        "\n",
        "            if special_day_str == 'Visarjan': headcount *= random.uniform(3.5, 5.0)\n",
        "            elif special_day_str == 'Pratishthapana': headcount *= random.uniform(2.0, 3.0)\n",
        "\n",
        "            if is_weekend == 1: # Logic now uses the binary flag\n",
        "                headcount *= random.uniform(1.4, 2.0)\n",
        "\n",
        "            if weather == 'Heavy Rain': headcount *= 0.4\n",
        "            elif weather == 'Light Rain': headcount *= 0.7\n",
        "\n",
        "            # Append the new binary columns to the data\n",
        "            all_data.append([\n",
        "                mandal_name, city, current_time, current_year, day_of_festival,\n",
        "                hour_of_day, is_weekend, is_special_day, weather, int(headcount)\n",
        "            ])\n",
        "\n",
        "    # Update the column names for the final DataFrame\n",
        "    columns = [\n",
        "        'mandal_name', 'city', 'datetime', 'year', 'day_of_festival', 'hour_of_day',\n",
        "        'is_weekend', 'is_special_day', 'weather', 'headcount'\n",
        "    ]\n",
        "    df = pd.DataFrame(all_data, columns=columns)\n",
        "\n",
        "    filename = f\"{mandal_name.lower().replace(' ', '_')}_binary_dataset.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Successfully created '{filename}' with {len(df)} rows.\")\n",
        "    # Show a preview of the first file generated\n",
        "    if mandal_name == mandals_to_generate[0][\"mandal_name\"]:\n",
        "        print(\"\\nPreview of the new format:\")\n",
        "        print(df.head())\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- Configuration for 5 Mandals ---\n",
        "mandals_to_generate = [\n",
        "    {\n",
        "        \"mandal_name\": \"Lalbaugcha Raja\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (10000, 25000),\n",
        "        \"yearly_growth_factor\": 1800\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Dagdusheth Halwai Ganpati\",\n",
        "        \"city\": \"Pune\",\n",
        "        \"base_headcount_range\": (8000, 20000),\n",
        "        \"yearly_growth_factor\": 1500\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Andhericha Raja\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (4000, 9000),\n",
        "        \"yearly_growth_factor\": 1000\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Kasba Ganpati\",\n",
        "        \"city\": \"Pune\",\n",
        "        \"base_headcount_range\": (3000, 7000),\n",
        "        \"yearly_growth_factor\": 700\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Siddhivinayak Temple\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (7000, 15000),\n",
        "        \"yearly_growth_factor\": 1200\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting dataset generation with binary columns...\")\n",
        "    for mandal_config in mandals_to_generate:\n",
        "        generate_mandal_dataset(**mandal_config)\n",
        "    print(\"All datasets have been generated successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h6WohSmV6VVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# --- Step 1: Combine All Datasets ---\n",
        "\n",
        "# Find all CSV files that end with '_binary_dataset.csv'\n",
        "file_pattern = '*_binary_dataset.csv'\n",
        "all_files = glob.glob(file_pattern)\n",
        "\n",
        "print(f\"Found {len(all_files)} files to combine:\")\n",
        "print(all_files)\n",
        "\n",
        "# Load each file into a DataFrame and store them in a list\n",
        "list_of_dfs = [pd.read_csv(f) for f in all_files]\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"\\nShape of the combined dataset before sorting: {combined_df.shape}\")\n",
        "\n",
        "# --- Step 2: Prepare and Sort the Combined Data ---\n",
        "\n",
        "# Convert 'datetime' column to a proper datetime object\n",
        "combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n",
        "\n",
        "# CRITICAL: Sort the entire DataFrame by datetime to ensure chronological order\n",
        "combined_df = combined_df.sort_values(by='datetime').reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCombined dataset has been sorted chronologically.\")\n",
        "print(\"Preview of the first few rows (start of 2010):\")\n",
        "print(combined_df.head())\n",
        "print(\"\\nPreview of the last few rows (end of 2024):\")\n",
        "print(combined_df.tail())\n",
        "\n",
        "\n",
        "# --- Step 3: Perform the Chronological Split ---\n",
        "\n",
        "# Define split points (70% train, 15% validation, 15% test)\n",
        "train_split_index = int(len(combined_df) * 0.70)\n",
        "validation_split_index = int(len(combined_df) * 0.85)\n",
        "\n",
        "# Create the splits\n",
        "train_data = combined_df.iloc[:train_split_index]\n",
        "validation_data = combined_df.iloc[train_split_index:validation_split_index]\n",
        "test_data = combined_df.iloc[validation_split_index:]\n",
        "\n",
        "print(\"\\n--- Data Split Complete ---\")\n",
        "print(f\"Training data shape:   {train_data.shape}\")\n",
        "print(f\"Validation data shape: {validation_data.shape}\")\n",
        "print(f\"Test data shape:       {test_data.shape}\")\n",
        "print(\"---\")\n",
        "print(f\"Training data dates:   {train_data['datetime'].min()} to {train_data['datetime'].max()}\")\n",
        "print(f\"Validation data dates: {validation_data['datetime'].min()} to {validation_data['datetime'].max()}\")\n",
        "print(f\"Test data dates:       {test_data['datetime'].min()} to {test_data['datetime'].max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-yyumdE5xq7",
        "outputId": "978ecf5f-8b08-447d-eadb-f3964c5a1e75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 files to combine:\n",
            "['andhericha_raja_binary_dataset.csv', 'siddhivinayak_temple_binary_dataset.csv', 'dagdusheth_halwai_ganpati_binary_dataset.csv', 'lalbaugcha_raja_binary_dataset.csv', 'kasba_ganpati_binary_dataset.csv']\n",
            "\n",
            "Shape of the combined dataset before sorting: (19800, 10)\n",
            "\n",
            "Combined dataset has been sorted chronologically.\n",
            "Preview of the first few rows (start of 2010):\n",
            "                 mandal_name    city   datetime  year  day_of_festival  \\\n",
            "0            Andhericha Raja  Mumbai 2010-09-05  2010                1   \n",
            "1  Dagdusheth Halwai Ganpati    Pune 2010-09-05  2010                1   \n",
            "2            Lalbaugcha Raja  Mumbai 2010-09-05  2010                1   \n",
            "3       Siddhivinayak Temple  Mumbai 2010-09-05  2010                1   \n",
            "4              Kasba Ganpati    Pune 2010-09-05  2010                1   \n",
            "\n",
            "   hour_of_day  is_weekend  is_special_day     weather  headcount  \n",
            "0            0           1               1       Sunny      24036  \n",
            "1            0           1               1       Sunny      44587  \n",
            "2            0           1               1       Humid      79170  \n",
            "3            0           1               1  Light Rain      22566  \n",
            "4            0           1               1  Heavy Rain      11982  \n",
            "\n",
            "Preview of the last few rows (end of 2024):\n",
            "                     mandal_name    city            datetime  year  \\\n",
            "19795            Lalbaugcha Raja  Mumbai 2024-09-15 23:00:00  2024   \n",
            "19796       Siddhivinayak Temple  Mumbai 2024-09-15 23:00:00  2024   \n",
            "19797            Andhericha Raja  Mumbai 2024-09-15 23:00:00  2024   \n",
            "19798  Dagdusheth Halwai Ganpati    Pune 2024-09-15 23:00:00  2024   \n",
            "19799              Kasba Ganpati    Pune 2024-09-15 23:00:00  2024   \n",
            "\n",
            "       day_of_festival  hour_of_day  is_weekend  is_special_day     weather  \\\n",
            "19795               11           23           1               1       Humid   \n",
            "19796               11           23           1               1  Heavy Rain   \n",
            "19797               11           23           1               1  Heavy Rain   \n",
            "19798               11           23           1               1       Sunny   \n",
            "19799               11           23           1               1  Light Rain   \n",
            "\n",
            "       headcount  \n",
            "19795     920186  \n",
            "19796     310637  \n",
            "19797     200376  \n",
            "19798     874683  \n",
            "19799     251944  \n",
            "\n",
            "--- Data Split Complete ---\n",
            "Training data shape:   (13860, 10)\n",
            "Validation data shape: (2970, 10)\n",
            "Test data shape:       (2970, 10)\n",
            "---\n",
            "Training data dates:   2010-09-05 00:00:00 to 2020-09-10 11:00:00\n",
            "Validation data dates: 2020-09-10 12:00:00 to 2022-09-13 05:00:00\n",
            "Test data dates:       2022-09-13 06:00:00 to 2024-09-15 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assume 'train_data', 'validation_data', and 'test_data' are already created\n",
        "# from the previous step.\n",
        "\n",
        "# --- Step 1: Feature Engineering (Adding Lag Features) ---\n",
        "def create_features(df):\n",
        "    \"\"\"Create time series features based on datetime index.\"\"\"\n",
        "    df = df.copy()\n",
        "    # Create a 1-hour lag and a 24-hour lag\n",
        "    df['headcount_lag_1hr'] = df.groupby('mandal_name')['headcount'].shift(1)\n",
        "    df['headcount_lag_24hr'] = df.groupby('mandal_name')['headcount'].shift(24)\n",
        "    return df\n",
        "\n",
        "train_featured = create_features(train_data)\n",
        "validation_featured = create_features(validation_data)\n",
        "test_featured = create_features(test_data)\n",
        "\n",
        "\n",
        "# --- Step 2: Preprocessing ---\n",
        "\n",
        "# Define which columns are features and which is the target\n",
        "TARGET = 'headcount'\n",
        "# Drop the original datetime and any rows with missing lag values\n",
        "FEATURES = ['year', 'day_of_festival', 'hour_of_day', 'is_weekend',\n",
        "            'is_special_day', 'mandal_name', 'city', 'weather',\n",
        "            'headcount_lag_1hr', 'headcount_lag_24hr']\n",
        "\n",
        "# Create X (features) and y (target) sets\n",
        "X_train = train_featured[FEATURES].dropna()\n",
        "y_train = train_featured.loc[X_train.index][TARGET]\n",
        "\n",
        "X_val = validation_featured[FEATURES].dropna()\n",
        "y_val = validation_featured.loc[X_val.index][TARGET]\n",
        "\n",
        "X_test = test_featured[FEATURES].dropna()\n",
        "y_test = test_featured.loc[X_test.index][TARGET]\n",
        "\n",
        "\n",
        "# One-Hot Encode categorical features\n",
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Align columns - ensures val/test sets have the same columns as the train set\n",
        "# after one-hot encoding, filling missing ones with 0.\n",
        "train_cols = X_train.columns\n",
        "X_val = X_val.reindex(columns=train_cols, fill_value=0)\n",
        "X_test = X_test.reindex(columns=train_cols, fill_value=0)\n",
        "\n",
        "\n",
        "# --- Step 3 & 4: Model Training and Evaluation ---\n",
        "\n",
        "# Initialize and train the LightGBM Regressor model\n",
        "model = lgb.LGBMRegressor(objective='mae',\n",
        "                          metric='mae',\n",
        "                          n_estimators=1000,\n",
        "                          n_jobs=-1,\n",
        "                          learning_rate=0.05,\n",
        "                          random_state=42)\n",
        "\n",
        "print(\"Training the LightGBM model...\")\n",
        "model.fit(X_train, y_train,\n",
        "          eval_set=[(X_val, y_val)],\n",
        "          eval_metric='mae',\n",
        "          callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "# Make predictions on the validation data\n",
        "val_predictions = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, val_predictions)\n",
        "print(f\"\\nModel training complete.\")\n",
        "print(f\"The Mean Absolute Error (MAE) on the validation set is: {mae:,.2f}\")\n",
        "print(f\"This means the model's predictions are, on average, off by about {int(round(mae, -2))} people.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNQd2voY6l1F",
        "outputId": "792e0bf9-2748-4d25-fed4-962b72d7e6b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the LightGBM model...\n",
            "\n",
            "Model training complete.\n",
            "The Mean Absolute Error (MAE) on the validation set is: 9,309.08\n",
            "This means the model's predictions are, on average, off by about 9300 people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick implementation script - add this to your existing notebook\n",
        "# This uses your existing train_data, validation_data, test_data\n",
        "\n",
        "# Install required packages (run this first if needed)\n",
        "# !pip install optuna xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_enhanced_features(df):\n",
        "    \"\"\"Create enhanced features from your existing data.\"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['mandal_name', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "    # Basic lag features (improved)\n",
        "    for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "        df[f'headcount_lag_{lag}h'] = df.groupby('mandal_name')['headcount'].shift(lag)\n",
        "\n",
        "    # Rolling statistics\n",
        "    for window in [3, 6, 12, 24]:\n",
        "        df[f'headcount_rolling_mean_{window}h'] = df.groupby('mandal_name')['headcount'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
        "        )\n",
        "        df[f'headcount_rolling_std_{window}h'] = df.groupby('mandal_name')['headcount'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).std().shift(1)\n",
        "        )\n",
        "\n",
        "    # Cyclical time features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_festival'] / 11)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_festival'] / 11)\n",
        "\n",
        "    # Weather impact score\n",
        "    weather_impact = {\n",
        "        'Sunny': 1.0, 'Humid': 0.9, 'Cloudy': 0.85,\n",
        "        'Light Rain': 0.7, 'Heavy Rain': 0.4\n",
        "    }\n",
        "    df['weather_impact_score'] = df['weather'].map(weather_impact)\n",
        "\n",
        "    # Interaction features\n",
        "    df['hour_x_weekend'] = df['hour_of_day'] * df['is_weekend']\n",
        "    df['hour_x_special'] = df['hour_of_day'] * df['is_special_day']\n",
        "    df['weather_x_weekend'] = df['weather_impact_score'] * df['is_weekend']\n",
        "\n",
        "    # Peak hours\n",
        "    df['is_peak_hour'] = ((df['hour_of_day'] >= 18) & (df['hour_of_day'] <= 23)).astype(int)\n",
        "    df['is_late_night'] = ((df['hour_of_day'] >= 1) & (df['hour_of_day'] <= 5)).astype(int)\n",
        "\n",
        "    # Festival progress\n",
        "    df['festival_progress'] = df['day_of_festival'] / 11\n",
        "    df['days_to_visarjan'] = 11 - df['day_of_festival']\n",
        "\n",
        "    # City encoding\n",
        "    df['is_mumbai'] = (df['city'] == 'Mumbai').astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_data_for_training(train_data, validation_data, test_data):\n",
        "    \"\"\"Prepare data with enhanced features.\"\"\"\n",
        "    print(\"Creating enhanced features...\")\n",
        "\n",
        "    # Create features for all datasets\n",
        "    train_featured = create_enhanced_features(train_data)\n",
        "    val_featured = create_enhanced_features(validation_data)\n",
        "    test_featured = create_enhanced_features(test_data)\n",
        "\n",
        "    # Define feature columns\n",
        "    feature_cols = [\n",
        "        'year', 'day_of_festival', 'hour_of_day', 'is_weekend', 'is_special_day',\n",
        "        'weather_impact_score', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
        "        'hour_x_weekend', 'hour_x_special', 'weather_x_weekend',\n",
        "        'is_peak_hour', 'is_late_night', 'festival_progress', 'days_to_visarjan', 'is_mumbai'\n",
        "    ]\n",
        "\n",
        "    # Add lag and rolling features\n",
        "    lag_cols = [col for col in train_featured.columns if 'lag_' in col or 'rolling_' in col]\n",
        "    feature_cols.extend(lag_cols)\n",
        "\n",
        "    # Encode mandal names\n",
        "    le = LabelEncoder()\n",
        "    train_featured['mandal_encoded'] = le.fit_transform(train_featured['mandal_name'])\n",
        "    val_featured['mandal_encoded'] = le.transform(val_featured['mandal_name'])\n",
        "    test_featured['mandal_encoded'] = le.transform(test_featured['mandal_name'])\n",
        "    feature_cols.append('mandal_encoded')\n",
        "\n",
        "    # Prepare datasets\n",
        "    X_train = train_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "    X_val = val_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "    X_test = test_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "\n",
        "    y_train = train_featured['headcount']\n",
        "    y_val = val_featured['headcount']\n",
        "    y_test = test_featured['headcount']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    print(f\"Training features shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test, scaler\n",
        "\n",
        "def train_enhanced_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    \"\"\"Train the enhanced ensemble model.\"\"\"\n",
        "\n",
        "    # Individual models with better parameters\n",
        "    lgb_model = lgb.LGBMRegressor(\n",
        "        objective='regression', metric='mae', n_estimators=1000,\n",
        "        learning_rate=0.05, max_depth=8, num_leaves=100,\n",
        "        min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=-1\n",
        "    )\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=800, learning_rate=0.05, max_depth=8,\n",
        "        min_child_weight=5, subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_alpha=1.0, reg_lambda=1.0, random_state=42\n",
        "    )\n",
        "\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=15, min_samples_split=10,\n",
        "        min_samples_leaf=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create ensemble\n",
        "    ensemble = VotingRegressor([\n",
        "        ('lgb', lgb_model),\n",
        "        ('xgb', xgb_model),\n",
        "        ('rf', rf_model)\n",
        "    ])\n",
        "\n",
        "    print(\"Training enhanced ensemble model...\")\n",
        "    ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_pred = ensemble.predict(X_val)\n",
        "    val_mae = mean_absolute_error(y_val, val_pred)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "\n",
        "    print(f\"\\n=== Validation Results ===\")\n",
        "    print(f\"MAE: {val_mae:,.2f} (Previous: 8,793)\")\n",
        "    print(f\"RMSE: {val_rmse:,.2f}\")\n",
        "    print(f\"R²: {val_r2:.4f}\")\n",
        "    print(f\"Improvement: {8793 - val_mae:+.2f} MAE reduction\")\n",
        "\n",
        "    # Test set evaluation\n",
        "    test_pred = ensemble.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, test_pred)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"\\n=== Test Results ===\")\n",
        "    print(f\"MAE: {test_mae:,.2f}\")\n",
        "    print(f\"RMSE: {test_rmse:,.2f}\")\n",
        "    print(f\"R²: {test_r2:.4f}\")\n",
        "\n",
        "    # Feature importance from LightGBM\n",
        "    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': lgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\n=== Top 10 Important Features ===\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    return ensemble, feature_importance\n",
        "\n",
        "# Execute the enhanced model\n",
        "print(\"=== Enhanced Ganpati Festival Crowd Prediction ===\")\n",
        "\n",
        "# Prepare data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data_for_training(\n",
        "    train_data, validation_data, test_data\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model, importance = train_enhanced_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "\n",
        "print(f\"\\n=== Enhancement Complete ===\")\n",
        "print(\"Your model should now perform significantly better!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"- Advanced lag and rolling features\")\n",
        "print(\"- Cyclical time encoding\")\n",
        "print(\"- Weather impact scoring\")\n",
        "print(\"- Interaction features\")\n",
        "print(\"- Ensemble of 3 models\")\n",
        "print(\"- Robust scaling\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PH2E0zxFsEb",
        "outputId": "d52239df-70b9-4dff-bc02-e1a1651f6636"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Enhanced Ganpati Festival Crowd Prediction ===\n",
            "Creating enhanced features...\n",
            "Training features shape: (13860, 34)\n",
            "Number of features: 34\n",
            "Training enhanced ensemble model...\n",
            "\n",
            "=== Validation Results ===\n",
            "MAE: 8,834.51 (Previous: 8,793)\n",
            "RMSE: 20,412.18\n",
            "R²: 0.8917\n",
            "Improvement: -41.51 MAE reduction\n",
            "\n",
            "=== Test Results ===\n",
            "MAE: 11,364.67\n",
            "RMSE: 31,684.74\n",
            "R²: 0.8668\n",
            "\n",
            "=== Top 10 Important Features ===\n",
            "                       feature  importance\n",
            "5         weather_impact_score         694\n",
            "31  headcount_rolling_mean_24h         566\n",
            "18            headcount_lag_1h         525\n",
            "23           headcount_lag_24h         520\n",
            "2                  hour_of_day         500\n",
            "29  headcount_rolling_mean_12h         497\n",
            "21            headcount_lag_6h         462\n",
            "1              day_of_festival         444\n",
            "24           headcount_lag_48h         437\n",
            "22           headcount_lag_12h         433\n",
            "\n",
            "=== Enhancement Complete ===\n",
            "Your model should now perform significantly better!\n",
            "Key improvements:\n",
            "- Advanced lag and rolling features\n",
            "- Cyclical time encoding\n",
            "- Weather impact scoring\n",
            "- Interaction features\n",
            "- Ensemble of 3 models\n",
            "- Robust scaling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick implementation script - add this to your existing notebook\n",
        "# This uses your existing train_data, validation_data, test_data\n",
        "\n",
        "# Install required packages (run this first if needed)\n",
        "# !pip install optuna xgboost scikit-learn pandas numpy lightgbm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.multioutput import MultiOutputRegressor # Import MultiOutputRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_enhanced_features(df):\n",
        "    \"\"\"\n",
        "    Create enhanced features from your existing data, including the new turnover target.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['mandal_name', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "    # --- NEW: Create the turnover target variable ---\n",
        "    # This calculates the change in headcount for the next hour for each mandal\n",
        "    df['turnover_next_hour'] = df.groupby('mandal_name')['headcount'].transform(\n",
        "        lambda x: x.shift(-1) - x\n",
        "    )\n",
        "\n",
        "    # Basic lag features\n",
        "    for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "        df[f'headcount_lag_{lag}h'] = df.groupby('mandal_name')['headcount'].shift(lag)\n",
        "\n",
        "    # Rolling statistics\n",
        "    for window in [3, 6, 12, 24]:\n",
        "        df[f'headcount_rolling_mean_{window}h'] = df.groupby('mandal_name')['headcount'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
        "        )\n",
        "        df[f'headcount_rolling_std_{window}h'] = df.groupby('mandal_name')['headcount'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).std().shift(1)\n",
        "        )\n",
        "\n",
        "    # Cyclical time features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_festival'] / 11)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_festival'] / 11)\n",
        "\n",
        "    # Weather impact score\n",
        "    weather_impact = {\n",
        "        'Sunny': 1.0, 'Humid': 0.9, 'Cloudy': 0.85,\n",
        "        'Light Rain': 0.7, 'Heavy Rain': 0.4\n",
        "    }\n",
        "    df['weather_impact_score'] = df['weather'].map(weather_impact)\n",
        "\n",
        "    # Interaction features\n",
        "    df['hour_x_weekend'] = df['hour_of_day'] * df['is_weekend']\n",
        "    df['hour_x_special'] = df['hour_of_day'] * df['is_special_day']\n",
        "    df['weather_x_weekend'] = df['weather_impact_score'] * df['is_weekend']\n",
        "\n",
        "    # Peak hours\n",
        "    df['is_peak_hour'] = ((df['hour_of_day'] >= 18) & (df['hour_of_day'] <= 23)).astype(int)\n",
        "    df['is_late_night'] = ((df['hour_of_day'] >= 1) & (df['hour_of_day'] <= 5)).astype(int)\n",
        "\n",
        "    # Festival progress\n",
        "    df['festival_progress'] = df['day_of_festival'] / 11\n",
        "    df['days_to_visarjan'] = 11 - df['day_of_festival']\n",
        "\n",
        "    # City encoding\n",
        "    df['is_mumbai'] = (df['city'] == 'Mumbai').astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_data_for_training(train_data, validation_data, test_data):\n",
        "    \"\"\"Prepare data with enhanced features for multi-output training.\"\"\"\n",
        "    print(\"Creating enhanced features for headcount and turnover prediction...\")\n",
        "\n",
        "    # Create features for all datasets\n",
        "    train_featured = create_enhanced_features(train_data)\n",
        "    val_featured = create_enhanced_features(validation_data)\n",
        "    test_featured = create_enhanced_features(test_data)\n",
        "\n",
        "    # --- IMPORTANT: Drop rows where turnover cannot be calculated (the last entry for each mandal) ---\n",
        "    train_featured = train_featured.dropna(subset=['turnover_next_hour'])\n",
        "    val_featured = val_featured.dropna(subset=['turnover_next_hour'])\n",
        "    test_featured = test_featured.dropna(subset=['turnover_next_hour'])\n",
        "\n",
        "    # Define feature columns\n",
        "    feature_cols = [\n",
        "        'year', 'day_of_festival', 'hour_of_day', 'is_weekend', 'is_special_day',\n",
        "        'weather_impact_score', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
        "        'hour_x_weekend', 'hour_x_special', 'weather_x_weekend',\n",
        "        'is_peak_hour', 'is_late_night', 'festival_progress', 'days_to_visarjan', 'is_mumbai'\n",
        "    ]\n",
        "    lag_cols = [col for col in train_featured.columns if 'lag_' in col or 'rolling_' in col]\n",
        "    feature_cols.extend(lag_cols)\n",
        "\n",
        "    # Encode mandal names\n",
        "    le = LabelEncoder()\n",
        "    train_featured['mandal_encoded'] = le.fit_transform(train_featured['mandal_name'])\n",
        "    val_featured['mandal_encoded'] = le.transform(val_featured['mandal_name'])\n",
        "    test_featured['mandal_encoded'] = le.transform(test_featured['mandal_name'])\n",
        "    feature_cols.append('mandal_encoded')\n",
        "\n",
        "    # Define target columns\n",
        "    target_cols = ['headcount', 'turnover_next_hour']\n",
        "\n",
        "    # Prepare datasets\n",
        "    X_train = train_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "    X_val = val_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "    X_test = test_featured[feature_cols].fillna(method='ffill').fillna(0)\n",
        "\n",
        "    y_train = train_featured[target_cols]\n",
        "    y_val = val_featured[target_cols]\n",
        "    y_test = test_featured[target_cols]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    print(f\"Training features shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test, scaler\n",
        "\n",
        "def train_enhanced_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    \"\"\"Train the enhanced multi-output ensemble model.\"\"\"\n",
        "\n",
        "    # Define individual models (these will be used for each target)\n",
        "    lgb_model = lgb.LGBMRegressor(\n",
        "        objective='regression', metric='mae', n_estimators=1000,\n",
        "        learning_rate=0.05, max_depth=8, num_leaves=100,\n",
        "        min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=-1\n",
        "    )\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=800, learning_rate=0.05, max_depth=8,\n",
        "        min_child_weight=5, subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_alpha=1.0, reg_lambda=1.0, random_state=42\n",
        "    )\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=15, min_samples_split=10,\n",
        "        min_samples_leaf=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create the base ensemble\n",
        "    base_ensemble = VotingRegressor([\n",
        "        ('lgb', lgb_model), ('xgb', xgb_model), ('rf', rf_model)\n",
        "    ])\n",
        "\n",
        "    # --- NEW: Wrap the ensemble in a MultiOutputRegressor ---\n",
        "    multi_output_ensemble = MultiOutputRegressor(base_ensemble, n_jobs=-1)\n",
        "\n",
        "    print(\"Training enhanced multi-output ensemble model...\")\n",
        "    multi_output_ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # --- UPDATED: Evaluate on validation set for both targets ---\n",
        "    val_pred = multi_output_ensemble.predict(X_val)\n",
        "\n",
        "    # Unpack predictions and true values\n",
        "    y_val_headcount, y_val_turnover = y_val['headcount'], y_val['turnover_next_hour']\n",
        "    val_pred_headcount, val_pred_turnover = val_pred[:, 0], val_pred[:, 1]\n",
        "\n",
        "    print(f\"\\n=== Validation Results ===\")\n",
        "    print(\"--- Headcount Prediction ---\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_val_headcount, val_pred_headcount):,.2f} (Previous: 8,793)\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val_headcount, val_pred_headcount)):,.2f}\")\n",
        "    print(f\"R²: {r2_score(y_val_headcount, val_pred_headcount):.4f}\")\n",
        "\n",
        "    print(\"\\n--- Turnover Prediction ---\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_val_turnover, val_pred_turnover):,.2f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val_turnover, val_pred_turnover)):,.2f}\")\n",
        "    print(f\"R²: {r2_score(y_val_turnover, val_pred_turnover):.4f}\")\n",
        "\n",
        "    # --- UPDATED: Test set evaluation ---\n",
        "    test_pred = multi_output_ensemble.predict(X_test)\n",
        "    y_test_headcount, y_test_turnover = y_test['headcount'], y_test['turnover_next_hour']\n",
        "    test_pred_headcount, test_pred_turnover = test_pred[:, 0], test_pred[:, 1]\n",
        "\n",
        "    print(f\"\\n=== Test Results ===\")\n",
        "    print(\"--- Headcount Prediction ---\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_test_headcount, test_pred_headcount):,.2f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_headcount, test_pred_headcount)):,.2f}\")\n",
        "    print(f\"R²: {r2_score(y_test_headcount, test_pred_headcount):.4f}\")\n",
        "\n",
        "    print(\"\\n--- Turnover Prediction ---\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_test_turnover, test_pred_turnover):,.2f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_turnover, test_pred_turnover)):,.2f}\")\n",
        "    print(f\"R²: {r2_score(y_test_turnover, test_pred_turnover):.4f}\")\n",
        "\n",
        "    # Feature importance (based on the headcount prediction model)\n",
        "    # Note: MultiOutputRegressor fits one model per target. We inspect one of them.\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': multi_output_ensemble.estimators_[0].named_estimators_['lgb'].feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\n=== Top 10 Important Features (for Headcount) ===\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    return multi_output_ensemble, feature_importance\n",
        "\n",
        "# === Main Execution ===\n",
        "print(\"=== Enhanced Ganpati Festival Crowd & Turnover Prediction ===\")\n",
        "\n",
        "# Prepare data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data_for_training(\n",
        "    train_data, validation_data, test_data\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model, importance = train_enhanced_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "\n",
        "print(f\"\\n=== Enhancement Complete ===\")\n",
        "print(\"Your model now predicts both headcount and next-hour turnover!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"- Advanced lag and rolling features\")\n",
        "print(\"- Cyclical time encoding\")\n",
        "print(\"- Weather impact scoring & interaction features\")\n",
        "print(\"- Multi-output ensemble of 3 models\")\n",
        "print(\"- Robust scaling\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNY_HTd2tdA1",
        "outputId": "9b7d5cb0-4a9e-4511-bfd1-8ce14c9202af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Enhanced Ganpati Festival Crowd & Turnover Prediction ===\n",
            "Creating enhanced features for headcount and turnover prediction...\n",
            "Training features shape: (13855, 34)\n",
            "Number of features: 34\n",
            "Training enhanced multi-output ensemble model...\n",
            "\n",
            "=== Validation Results ===\n",
            "--- Headcount Prediction ---\n",
            "MAE: 8,855.95 (Previous: 8,793)\n",
            "RMSE: 20,410.04\n",
            "R²: 0.8919\n",
            "\n",
            "--- Turnover Prediction ---\n",
            "MAE: 16,665.18\n",
            "RMSE: 34,106.25\n",
            "R²: 0.4805\n",
            "\n",
            "=== Test Results ===\n",
            "--- Headcount Prediction ---\n",
            "MAE: 11,115.41\n",
            "RMSE: 30,521.13\n",
            "R²: 0.8675\n",
            "\n",
            "--- Turnover Prediction ---\n",
            "MAE: 21,340.27\n",
            "RMSE: 49,614.17\n",
            "R²: 0.4075\n",
            "\n",
            "=== Top 10 Important Features (for Headcount) ===\n",
            "                       feature  importance\n",
            "23           headcount_lag_24h        2379\n",
            "18            headcount_lag_1h        2317\n",
            "24           headcount_lag_48h        2187\n",
            "21            headcount_lag_6h        1864\n",
            "19            headcount_lag_2h        1783\n",
            "22           headcount_lag_12h        1682\n",
            "20            headcount_lag_3h        1629\n",
            "31  headcount_rolling_mean_24h        1614\n",
            "28    headcount_rolling_std_6h        1544\n",
            "27   headcount_rolling_mean_6h        1527\n",
            "\n",
            "=== Enhancement Complete ===\n",
            "Your model now predicts both headcount and next-hour turnover!\n",
            "Key improvements:\n",
            "- Advanced lag and rolling features\n",
            "- Cyclical time encoding\n",
            "- Weather impact scoring & interaction features\n",
            "- Multi-output ensemble of 3 models\n",
            "- Robust scaling\n"
          ]
        }
      ]
    }
  ]
}
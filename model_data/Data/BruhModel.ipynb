{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br6jzJFiEclR",
        "outputId": "ecd3829f-6f11-4b22-936d-e7290628c83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset generation with binary columns...\n",
            "--- Generating dataset for: Lalbaugcha Raja, Mumbai ---\n",
            "Successfully created 'lalbaugcha_raja_binary_dataset.csv' with 3960 rows.\n",
            "\n",
            "Preview of the new format:\n",
            "       mandal_name    city            datetime  year  day_of_festival  \\\n",
            "0  Lalbaugcha Raja  Mumbai 2010-09-05 00:00:00  2010                1   \n",
            "1  Lalbaugcha Raja  Mumbai 2010-09-05 01:00:00  2010                1   \n",
            "2  Lalbaugcha Raja  Mumbai 2010-09-05 02:00:00  2010                1   \n",
            "3  Lalbaugcha Raja  Mumbai 2010-09-05 03:00:00  2010                1   \n",
            "4  Lalbaugcha Raja  Mumbai 2010-09-05 04:00:00  2010                1   \n",
            "\n",
            "   hour_of_day  is_weekend  is_special_day     weather  headcount  \n",
            "0            0           1               1       Sunny      35350  \n",
            "1            1           1               1       Humid      19998  \n",
            "2            2           1               1  Heavy Rain      10743  \n",
            "3            3           1               1  Heavy Rain       3415  \n",
            "4            4           1               1  Heavy Rain       7000  \n",
            "\n",
            "\n",
            "--- Generating dataset for: Dagdusheth Halwai Ganpati, Pune ---\n",
            "Successfully created 'dagdusheth_halwai_ganpati_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Andhericha Raja, Mumbai ---\n",
            "Successfully created 'andhericha_raja_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Kasba Ganpati, Pune ---\n",
            "Successfully created 'kasba_ganpati_binary_dataset.csv' with 3960 rows.\n",
            "--- Generating dataset for: Siddhivinayak Temple, Mumbai ---\n",
            "Successfully created 'siddhivinayak_temple_binary_dataset.csv' with 3960 rows.\n",
            "All datasets have been generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- Main Generation Function ---\n",
        "def generate_mandal_dataset(mandal_name, city, base_headcount_range, yearly_growth_factor, start_year=2010, num_years=15):\n",
        "    \"\"\"\n",
        "    Generates a realistic dummy dataset with binary flags for weekend and special days.\n",
        "    \"\"\"\n",
        "    print(f\"--- Generating dataset for: {mandal_name}, {city} ---\")\n",
        "\n",
        "    FESTIVAL_DURATION_DAYS = 11\n",
        "    weather_options = ['Humid', 'Cloudy', 'Sunny', 'Light Rain', 'Heavy Rain']\n",
        "    all_data = []\n",
        "    total_hours = FESTIVAL_DURATION_DAYS * 24\n",
        "\n",
        "    for year_offset in range(num_years):\n",
        "        current_year = start_year + year_offset\n",
        "        festival_start_date = datetime(current_year, 9, 5, 0, 0, 0)\n",
        "\n",
        "        for hour_offset in range(total_hours):\n",
        "            current_time = festival_start_date + timedelta(hours=hour_offset)\n",
        "\n",
        "            day_of_festival = (current_time - festival_start_date).days + 1\n",
        "            hour_of_day = current_time.hour\n",
        "\n",
        "            # --- MODIFICATION START ---\n",
        "\n",
        "            # 1. Determine day of week for headcount logic, then create binary flag\n",
        "            day_of_week_str = current_time.strftime('%A')\n",
        "            is_weekend = 1 if day_of_week_str in ['Saturday', 'Sunday'] else 0\n",
        "\n",
        "            # 2. Determine special day type for headcount logic, then create binary flag\n",
        "            if day_of_festival == 1: special_day_str = 'Pratishthapana'\n",
        "            elif day_of_festival == 11: special_day_str = 'Visarjan'\n",
        "            elif day_of_festival in [5, 7, 10]: special_day_str = 'Key Day'\n",
        "            else: special_day_str = 'Regular Day'\n",
        "            is_special_day = 1 if special_day_str != 'Regular Day' else 0\n",
        "\n",
        "            # --- MODIFICATION END ---\n",
        "\n",
        "            weather = random.choice(weather_options)\n",
        "\n",
        "            # Headcount Simulation (uses the original string variables for logic)\n",
        "            base_headcount = random.randint(*base_headcount_range)\n",
        "            base_headcount += (current_year - start_year) * yearly_growth_factor\n",
        "            headcount = float(base_headcount)\n",
        "\n",
        "            if 18 <= hour_of_day <= 23: headcount *= random.uniform(2.5, 4.0)\n",
        "            elif 1 <= hour_of_day <= 5: headcount *= random.uniform(0.1, 0.4)\n",
        "\n",
        "            if special_day_str == 'Visarjan': headcount *= random.uniform(3.5, 5.0)\n",
        "            elif special_day_str == 'Pratishthapana': headcount *= random.uniform(2.0, 3.0)\n",
        "\n",
        "            if is_weekend == 1: # Logic now uses the binary flag\n",
        "                headcount *= random.uniform(1.4, 2.0)\n",
        "\n",
        "            if weather == 'Heavy Rain': headcount *= 0.4\n",
        "            elif weather == 'Light Rain': headcount *= 0.7\n",
        "\n",
        "            # Append the new binary columns to the data\n",
        "            all_data.append([\n",
        "                mandal_name, city, current_time, current_year, day_of_festival,\n",
        "                hour_of_day, is_weekend, is_special_day, weather, int(headcount)\n",
        "            ])\n",
        "\n",
        "    # Update the column names for the final DataFrame\n",
        "    columns = [\n",
        "        'mandal_name', 'city', 'datetime', 'year', 'day_of_festival', 'hour_of_day',\n",
        "        'is_weekend', 'is_special_day', 'weather', 'headcount'\n",
        "    ]\n",
        "    df = pd.DataFrame(all_data, columns=columns)\n",
        "\n",
        "    filename = f\"{mandal_name.lower().replace(' ', '_')}_binary_dataset.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Successfully created '{filename}' with {len(df)} rows.\")\n",
        "    # Show a preview of the first file generated\n",
        "    if mandal_name == mandals_to_generate[0][\"mandal_name\"]:\n",
        "        print(\"\\nPreview of the new format:\")\n",
        "        print(df.head())\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- Configuration for 5 Mandals ---\n",
        "mandals_to_generate = [\n",
        "    {\n",
        "        \"mandal_name\": \"Lalbaugcha Raja\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (10000, 25000),\n",
        "        \"yearly_growth_factor\": 1800\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Dagdusheth Halwai Ganpati\",\n",
        "        \"city\": \"Pune\",\n",
        "        \"base_headcount_range\": (8000, 20000),\n",
        "        \"yearly_growth_factor\": 1500\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Andhericha Raja\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (4000, 9000),\n",
        "        \"yearly_growth_factor\": 1000\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Kasba Ganpati\",\n",
        "        \"city\": \"Pune\",\n",
        "        \"base_headcount_range\": (3000, 7000),\n",
        "        \"yearly_growth_factor\": 700\n",
        "    },\n",
        "    {\n",
        "        \"mandal_name\": \"Siddhivinayak Temple\",\n",
        "        \"city\": \"Mumbai\",\n",
        "        \"base_headcount_range\": (7000, 15000),\n",
        "        \"yearly_growth_factor\": 1200\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting dataset generation with binary columns...\")\n",
        "    for mandal_config in mandals_to_generate:\n",
        "        generate_mandal_dataset(**mandal_config)\n",
        "    print(\"All datasets have been generated successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h6WohSmV6VVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# --- Step 1: Combine All Datasets ---\n",
        "\n",
        "# Find all CSV files that end with '_binary_dataset.csv'\n",
        "file_pattern = '*_binary_dataset.csv'\n",
        "all_files = glob.glob(file_pattern)\n",
        "\n",
        "print(f\"Found {len(all_files)} files to combine:\")\n",
        "print(all_files)\n",
        "\n",
        "# Load each file into a DataFrame and store them in a list\n",
        "list_of_dfs = [pd.read_csv(f) for f in all_files]\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"\\nShape of the combined dataset before sorting: {combined_df.shape}\")\n",
        "\n",
        "# --- Step 2: Prepare and Sort the Combined Data ---\n",
        "\n",
        "# Convert 'datetime' column to a proper datetime object\n",
        "combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n",
        "\n",
        "# CRITICAL: Sort the entire DataFrame by datetime to ensure chronological order\n",
        "combined_df = combined_df.sort_values(by='datetime').reset_index(drop=True)\n",
        "\n",
        "print(\"\\nCombined dataset has been sorted chronologically.\")\n",
        "print(\"Preview of the first few rows (start of 2010):\")\n",
        "print(combined_df.head())\n",
        "print(\"\\nPreview of the last few rows (end of 2024):\")\n",
        "print(combined_df.tail())\n",
        "\n",
        "\n",
        "# --- Step 3: Perform the Chronological Split ---\n",
        "\n",
        "# Define split points (70% train, 15% validation, 15% test)\n",
        "train_split_index = int(len(combined_df) * 0.70)\n",
        "validation_split_index = int(len(combined_df) * 0.85)\n",
        "\n",
        "# Create the splits\n",
        "train_data = combined_df.iloc[:train_split_index]\n",
        "validation_data = combined_df.iloc[train_split_index:validation_split_index]\n",
        "test_data = combined_df.iloc[validation_split_index:]\n",
        "\n",
        "print(\"\\n--- Data Split Complete ---\")\n",
        "print(f\"Training data shape:   {train_data.shape}\")\n",
        "print(f\"Validation data shape: {validation_data.shape}\")\n",
        "print(f\"Test data shape:       {test_data.shape}\")\n",
        "print(\"---\")\n",
        "print(f\"Training data dates:   {train_data['datetime'].min()} to {train_data['datetime'].max()}\")\n",
        "print(f\"Validation data dates: {validation_data['datetime'].min()} to {validation_data['datetime'].max()}\")\n",
        "print(f\"Test data dates:       {test_data['datetime'].min()} to {test_data['datetime'].max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-yyumdE5xq7",
        "outputId": "d70f5bec-ff99-4fb4-f3b6-e31c51a57c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 files to combine:\n",
            "['siddhivinayak_temple_binary_dataset.csv', 'andhericha_raja_binary_dataset.csv', 'dagdusheth_halwai_ganpati_binary_dataset.csv', 'kasba_ganpati_binary_dataset.csv', 'lalbaugcha_raja_binary_dataset.csv']\n",
            "\n",
            "Shape of the combined dataset before sorting: (19800, 10)\n",
            "\n",
            "Combined dataset has been sorted chronologically.\n",
            "Preview of the first few rows (start of 2010):\n",
            "                 mandal_name    city   datetime  year  day_of_festival  \\\n",
            "0       Siddhivinayak Temple  Mumbai 2010-09-05  2010                1   \n",
            "1  Dagdusheth Halwai Ganpati    Pune 2010-09-05  2010                1   \n",
            "2              Kasba Ganpati    Pune 2010-09-05  2010                1   \n",
            "3            Andhericha Raja  Mumbai 2010-09-05  2010                1   \n",
            "4            Lalbaugcha Raja  Mumbai 2010-09-05  2010                1   \n",
            "\n",
            "   hour_of_day  is_weekend  is_special_day     weather  headcount  \n",
            "0            0           1               1  Heavy Rain      18502  \n",
            "1            0           1               1      Cloudy      38280  \n",
            "2            0           1               1       Sunny      25908  \n",
            "3            0           1               1  Heavy Rain      13913  \n",
            "4            0           1               1       Sunny      35350  \n",
            "\n",
            "Preview of the last few rows (end of 2024):\n",
            "                     mandal_name    city            datetime  year  \\\n",
            "19795              Kasba Ganpati    Pune 2024-09-15 23:00:00  2024   \n",
            "19796            Andhericha Raja  Mumbai 2024-09-15 23:00:00  2024   \n",
            "19797       Siddhivinayak Temple  Mumbai 2024-09-15 23:00:00  2024   \n",
            "19798  Dagdusheth Halwai Ganpati    Pune 2024-09-15 23:00:00  2024   \n",
            "19799            Lalbaugcha Raja  Mumbai 2024-09-15 23:00:00  2024   \n",
            "\n",
            "       day_of_festival  hour_of_day  is_weekend  is_special_day     weather  \\\n",
            "19795               11           23           1               1  Heavy Rain   \n",
            "19796               11           23           1               1  Light Rain   \n",
            "19797               11           23           1               1       Humid   \n",
            "19798               11           23           1               1  Heavy Rain   \n",
            "19799               11           23           1               1       Humid   \n",
            "\n",
            "       headcount  \n",
            "19795      94140  \n",
            "19796     377839  \n",
            "19797     706113  \n",
            "19798     335710  \n",
            "19799     958633  \n",
            "\n",
            "--- Data Split Complete ---\n",
            "Training data shape:   (13860, 10)\n",
            "Validation data shape: (2970, 10)\n",
            "Test data shape:       (2970, 10)\n",
            "---\n",
            "Training data dates:   2010-09-05 00:00:00 to 2020-09-10 11:00:00\n",
            "Validation data dates: 2020-09-10 12:00:00 to 2022-09-13 05:00:00\n",
            "Test data dates:       2022-09-13 06:00:00 to 2024-09-15 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assume 'train_data', 'validation_data', and 'test_data' are already created\n",
        "# from the previous step.\n",
        "\n",
        "# --- Step 1: Feature Engineering (Adding Lag Features) ---\n",
        "def create_features(df):\n",
        "    \"\"\"Create time series features based on datetime index.\"\"\"\n",
        "    df = df.copy()\n",
        "    # Create a 1-hour lag and a 24-hour lag\n",
        "    df['headcount_lag_1hr'] = df.groupby('mandal_name')['headcount'].shift(1)\n",
        "    df['headcount_lag_24hr'] = df.groupby('mandal_name')['headcount'].shift(24)\n",
        "    return df\n",
        "\n",
        "train_featured = create_features(train_data)\n",
        "validation_featured = create_features(validation_data)\n",
        "test_featured = create_features(test_data)\n",
        "\n",
        "\n",
        "# --- Step 2: Preprocessing ---\n",
        "\n",
        "# Define which columns are features and which is the target\n",
        "TARGET = 'headcount'\n",
        "# Drop the original datetime and any rows with missing lag values\n",
        "FEATURES = ['year', 'day_of_festival', 'hour_of_day', 'is_weekend',\n",
        "            'is_special_day', 'mandal_name', 'city', 'weather',\n",
        "            'headcount_lag_1hr', 'headcount_lag_24hr']\n",
        "\n",
        "# Create X (features) and y (target) sets\n",
        "X_train = train_featured[FEATURES].dropna()\n",
        "y_train = train_featured.loc[X_train.index][TARGET]\n",
        "\n",
        "X_val = validation_featured[FEATURES].dropna()\n",
        "y_val = validation_featured.loc[X_val.index][TARGET]\n",
        "\n",
        "X_test = test_featured[FEATURES].dropna()\n",
        "y_test = test_featured.loc[X_test.index][TARGET]\n",
        "\n",
        "\n",
        "# One-Hot Encode categorical features\n",
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Align columns - ensures val/test sets have the same columns as the train set\n",
        "# after one-hot encoding, filling missing ones with 0.\n",
        "train_cols = X_train.columns\n",
        "X_val = X_val.reindex(columns=train_cols, fill_value=0)\n",
        "X_test = X_test.reindex(columns=train_cols, fill_value=0)\n",
        "\n",
        "\n",
        "# --- Step 3 & 4: Model Training and Evaluation ---\n",
        "\n",
        "# Initialize and train the LightGBM Regressor model\n",
        "model = lgb.LGBMRegressor(objective='mae',\n",
        "                          metric='mae',\n",
        "                          n_estimators=1000,\n",
        "                          n_jobs=-1,\n",
        "                          learning_rate=0.05,\n",
        "                          random_state=42)\n",
        "\n",
        "print(\"Training the LightGBM model...\")\n",
        "model.fit(X_train, y_train,\n",
        "          eval_set=[(X_val, y_val)],\n",
        "          eval_metric='mae',\n",
        "          callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "# Make predictions on the validation data\n",
        "val_predictions = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_val, val_predictions)\n",
        "print(f\"\\nModel training complete.\")\n",
        "print(f\"The Mean Absolute Error (MAE) on the validation set is: {mae:,.2f}\")\n",
        "print(f\"This means the model's predictions are, on average, off by about {int(round(mae, -2))} people.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNQd2voY6l1F",
        "outputId": "4df0aef7-a29d-4400-cb72-1e065517c623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the LightGBM model...\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 580\n",
            "[LightGBM] [Info] Number of data points in the train set: 13740, number of used features: 16\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 16749.500000\n",
            "\n",
            "Model training complete.\n",
            "The Mean Absolute Error (MAE) on the validation set is: 8,793.75\n",
            "This means the model's predictions are, on average, off by about 8800 people.\n"
          ]
        }
      ]
    }
  ]
}